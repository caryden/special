# Evaluation Methodology: Special Skill Translation Experiments

## Objective

Measure whether a special skill (annotated reference implementation with layered specs)
produces better AI-generated code than alternative source formats, across multiple target
languages and library complexities.

"Better" means: higher first-pass correctness, fewer tokens consumed, fewer iterations
to full test pass, and more idiomatic target-language output.

## Experimental Design

### Independent Variables

**Source format** — what the agent receives as input:

| Format | Label | Description |
|--------|-------|-------------|
| Special skill reference | `REF` | Annotated TypeScript source + colocated tests |
| Markdown + YAML spec | `SPEC` | Behavioral spec + test vectors (dbreunig/whenwords style) |
| Natural language prompt | `PROMPT` | Plain English description of the library |

For whenwords specifically, `SPEC` is the actual whenwords SPEC.md + tests.yaml from
dbreunig/whenwords. For future libraries, an equivalent spec document should be written
for fair comparison.

**Target language** — what the agent generates:

| Language | Rationale |
|----------|-----------|
| Python | Highest LLM familiarity; baseline for "easy" translation |
| Rust | Strong type system, different paradigm (ownership, Result types) |
| Go | Minimal language, no generics (pre-1.18 style), different error model |
| C# | The language we considered as donor — how well does it receive? |
| Swift | Less training data, strict typing, different stdlib conventions |

Start with Python and Rust for whenwords. Add others as the methodology stabilizes.

**Library complexity** — staged progression:

| Stage | Library | Characteristics | Status |
|-------|---------|-----------------|--------|
| 1 | whenwords | 5 pure leaf nodes, no inter-node deps, string/date manipulation | Done |
| 2 | mathexpr | 6 nodes, DAG with shared types, parser/evaluator pipeline | Done |
| 3 | optimize | 10 nodes, numerical algorithms, cross-library validation | Done |
| 4 | TBD — extracted from existing | Real library subset (like the Optim.jl case) | Planned |

### Dependent Variables (Metrics)

#### Primary Metrics

**M1: First-pass test pass rate**
- Run the agent once with the source format. Run the generated tests.
- Metric: `tests_passed / tests_total` as a percentage.
- This is the most important metric. A high first-pass rate means the format
  communicates intent effectively.

**M2: Iterations to 100% pass**
- If first pass isn't 100%, allow the agent to iterate (read failures, fix code).
- Metric: number of agent turns (tool calls / responses) until all tests pass, or
  a cap of N iterations (suggest N=5).
- Record "did not converge" if 100% is not reached within the cap.

**M3: Input token count**
- Total tokens in the source material provided to the agent.
- For `REF`: the TypeScript source files + test files for the requested subgraph.
- For `SPEC`: the spec document + test vectors.
- For `PROMPT`: the natural language prompt.
- Measure using the model's tokenizer or a close approximation.

**M4: Output token count**
- Total tokens generated by the agent across all iterations.
- Lower is better (for equivalent correctness) — indicates the format requires
  less agent reasoning and exploration.

**M5: Total token cost**
- `input_tokens + output_tokens` across all iterations.
- The practical cost metric — what you'd actually pay.

#### Secondary Metrics

**M6: Idiomatic quality score** (manual review, 1-5 scale)
- Does the generated code look like a human wrote it in the target language?
- Does it use target-language conventions (naming, error handling, module structure)?
- Scored by manual review. Define a rubric per target language.

**M7: Test translation accuracy**
- Were the tests themselves translated correctly?
- Metric: did the agent preserve all test cases, or did it drop/modify any?
- Binary per test case: preserved / dropped / modified.

**M8: Dependency count**
- How many external dependencies does the generated code introduce?
- The special skill promise is zero external deps. Measure whether the agent adds any.

### Controls

**Fresh context per run.** Each experimental run must start with a clean agent session —
no prior conversation, no cached context. The agent receives only:
1. A system prompt establishing it as a coding agent
2. The source material (REF, SPEC, or PROMPT)
3. A task instruction: "Generate a <target-language> implementation of <library/node>.
   Run the tests and ensure they all pass."

**Identical task instruction.** The task phrasing must be identical across formats for
the same target language. Only the source material changes.

**Same model and parameters.** All runs use the same model (e.g., Claude Opus 4.5),
same temperature, same max tokens. Record the exact model ID.

**Same test vectors.** Regardless of source format, the expected test behavior is
identical. For `PROMPT` format, the test vectors must be provided separately (since
the prompt alone may not specify exact outputs).

### Procedure

For each combination of (source format × target language):

1. **Setup**: Create a clean working directory with the target language's project scaffold
   (e.g., `cargo init` for Rust, `pip init` for Python).

2. **Provide source material**: Give the agent the source material for the requested format.
   - `REF`: The special skill reference source files (implementation + tests).
   - `SPEC`: The markdown spec + YAML test vectors.
   - `PROMPT`: A natural language description of the library.

3. **Task instruction**: Ask the agent to generate the implementation and tests in the
   target language and verify all tests pass.

4. **Record first-pass results**: After the agent's first complete attempt, run the tests
   independently (outside the agent) and record M1 (pass rate).

5. **Allow iteration**: If not 100%, let the agent see failures and iterate. Record each
   iteration's pass rate and token usage.

6. **Cap iterations**: Stop after 5 iterations or 100% pass, whichever comes first.

7. **Manual review**: Score M6 (idiomatic quality) on the final output.

8. **Record all metrics**: Log everything to the results file.

### Repetition

Each (format × language) combination should be run **3 times** to account for
non-deterministic model output. Report mean and range for each metric.

## Data Collection

### Run Log Format

Each run produces a structured record:

```yaml
run_id: "whenwords-ref-python-001"
timestamp: "2026-02-01T12:00:00Z"
model: "claude-opus-4-5-20251101"

source:
  format: "REF"         # REF | SPEC | PROMPT
  library: "whenwords"
  nodes_requested: ["time-ago", "duration", "parse-duration", "human-date", "date-range"]
  input_tokens: 4230    # measured

target:
  language: "python"
  test_framework: "pytest"

results:
  first_pass:
    tests_total: 124
    tests_passed: 120
    pass_rate: 0.968
  iterations_to_100: 2      # or "did_not_converge"
  total_output_tokens: 8500
  total_input_tokens: 6200
  total_tokens: 14700
  external_dependencies: 0
  tests_dropped: 0
  tests_modified: 0

idiomatic_score: 4           # 1-5, manual review
notes: "Agent used dataclasses for options, pytest.mark.parametrize for test.each"
```

### Results Summary Table

Aggregate across runs:

```
Format | Language | First-Pass % | Iters to 100% | Input Tok | Output Tok | Idiomatic
-------|----------|-------------|---------------|-----------|-----------|----------
REF    | Python   | 96.8 ± 2.1  | 1.7 ± 0.6    | 4230      | 8500 ± 800 | 4.3
SPEC   | Python   | 91.2 ± 3.5  | 2.3 ± 0.6    | 3100      | 9200 ± 1100| 3.7
PROMPT | Python   | 78.0 ± 8.2  | 4.0 ± 1.0    | 800       | 12000 ± 2k | 3.0
REF    | Rust     | ...         | ...           | ...       | ...        | ...
```

## Preparing the Source Material

### REF format (special skill reference)

Already exists: `skills/when-words/reference/src/`. No preparation needed.

Provide the agent with all `.ts` source and test files for the requested nodes.

### SPEC format (Markdown + YAML)

Use the actual dbreunig/whenwords SPEC.md and tests.yaml. This is the fairest
comparison since it's the original source that inspired the project.

If those aren't available, create an equivalent spec document that describes
the same behavior without providing implementation code.

### PROMPT format (Natural language)

Write a concise natural language description of each function. No code, no test vectors,
no structured data. Just English:

```
Implement a relative time formatting library with five functions:

1. timeAgo(timestamp, reference) - returns strings like "3 hours ago" or "in 2 days".
   Uses thresholds: 0-44s = "just now", 45-89s = "1 minute ago", 90s-44min = "N minutes ago",
   45-89min = "1 hour ago", ...

2. duration(seconds, options?) - formats seconds as "2 hours, 30 minutes" or compact "2h 30m".
   ...
```

The prompt should be detailed enough to be implementable but must not include code
or structured test vectors. This represents the baseline: "what if you just described
what you wanted?"

For the PROMPT format, test vectors must still be provided *separately* after
generation, to measure correctness. The agent doesn't see them during generation.

## Results to Date

### Stage 1: whenwords (5 nodes, no inter-node deps)

9 runs: 3 formats (REF, SPEC, PROMPT) × 3 languages (Python, Rust, Go).
See `results/whenwords-experiment-results.md` for full results.

- REF format produced the highest first-pass accuracy
- PROMPT format diverged on 6/116 Python tests (ambiguous threshold behavior)
- SPEC format was competitive with REF on simple string/date operations
- All REF translations achieved 100% cross-validation pass rate

### Stage 2: mathexpr (6 nodes, DAG with shared types)

12 runs: 4 formats (REF, SPEC, PROMPT, SKILL) × 3 languages.
See `results/mathexpr-experiment-results.md` for full results.

- All formats achieved 100% test pass rate (parser/evaluator domain is well-understood)
- SKILL format designed based on Stage 1-2 findings: hybrid of SPEC + REF with progressive disclosure
- Confirmed that `@depends-on` and topological ordering help with DAG translation

### Stage 3: optimize (10 nodes, numerical algorithms) — partial

3 runs: SKILL format × 3 languages (Python, Rust, Go), nelder-mead subset only.
See `results/optimize-nelder-mead-results.md` for full results.

- **108/108 tests pass** across all 3 languages
- **De-bundling confirmed**: subset (3 of 10 nodes) extracted and translated cleanly
- **Progressive disclosure validated**: all agents used all 4 layers (skill.md → spec → hints → reference)
- **3 spec ambiguities identified** via translation feedback (see `draft-issues/`)
- Cross-validated against scipy v1.17.0 (empirical) and Optim.jl v2.0.0 (from source)

### Format evolution

Based on these results, the **skill format is the canonical approach** going forward:

| Format | First tested | Current status |
|--------|-------------|----------------|
| REF | Stage 1 | Superseded by SKILL (still available as Layer 4) |
| SPEC | Stage 1 | Incorporated into SKILL as Layer 2 (spec.md) |
| PROMPT | Stage 1 | Not recommended for production use |
| SKILL | Stage 2 | **Canonical** — progressive disclosure with all layers |

## Scaling to Complex Libraries

The whenwords experiment was the methodology shakedown. Stages 2 and 3 confirmed
the approach scales. Each subsequent stage increased complexity along specific
dimensions:

### Stage 2: Library with Dependencies (completed: mathexpr)

**Completed.** mathexpr — a math expression parser/evaluator with 6 nodes:
- Nodes form a DAG: token-types → parser → evaluator (shared types across nodes)
- Tested with all 4 formats (REF, SPEC, PROMPT, SKILL) × 3 languages
- Led to the design of the skill format (progressive disclosure)

### Stage 3: Algorithmic Library (completed: optimize)

**Completed.** optimize — a numerical optimization library with 10 nodes:
- Complex algorithms: Nelder-Mead, gradient descent, BFGS, L-BFGS
- Strong inter-node dependencies (line-search, finite-diff shared across methods)
- Cross-validated against scipy v1.17.0 and Optim.jl v2.0.0
- Subset translation (3 of 10 nodes) confirmed de-bundling hypothesis
- 11-library survey comparing defaults and algorithms across ecosystems

### Stage 4: Extracted from Existing Code (not yet attempted)

Take a real, existing library and extract a special skill from it:
- Run the `create-special-skill` skill on an existing codebase
- Measure the effort to annotate vs. rewrite
- Test translation quality against the original library's test suite

This tests incremental adoption — the most likely real-world path.

### What to Watch For

As complexity increases, track how each metric degrades:
- Does first-pass accuracy drop faster for `REF` or `SPEC` as complexity grows?
- Do `@depends-on` annotations help the agent maintain coherence across nodes?
- Do `@hint` tags prevent common translation mistakes?
- At what complexity level does the `PROMPT` format become unusable?

These degradation curves are the real evidence for or against the special skill hypothesis.

## Running the Experiments

### Prerequisites

- Claude Code CLI (or API access for scripted runs)
- Target language toolchains: Python (pytest), Rust (cargo), Go, C# (dotnet), Swift
- Token counting: use the Anthropic API's `usage` field or tiktoken approximation
- A script to automate fresh-context runs (see below)

### Automation

For repeatability, each run should be scriptable:

```bash
# Pseudocode for a single run
mkdir -p experiments/<run-id>
cd experiments/<run-id>

# Copy source material for the format being tested
cp -r <source-material> ./input/

# Set up target language scaffold
<target-language-init-command>

# Run agent in fresh context with task instruction
# Record full transcript, token usage, and test results
claude --new-session \
  --input ./input/ \
  --task "Generate a <language> implementation of whenwords. Run tests until all pass." \
  --transcript ./transcript.json

# Run tests independently
<target-test-command> > ./test-results.txt

# Extract metrics
./extract-metrics.sh ./transcript.json ./test-results.txt > ./metrics.yaml
```

The exact automation approach will depend on available tooling. Manual runs
are acceptable for the whenwords experiment; automation becomes necessary
at stage 2+.

## Reporting

After each experiment stage, produce a summary that includes:

1. **Results table** — all metrics, all formats, all languages
2. **Key findings** — what surprised, what confirmed, what broke
3. **Methodology adjustments** — what to change for the next stage
4. **Decision impact** — does the data support, challenge, or refine the special skill hypothesis?

Store results in `experiments/<library-name>/results.md` alongside the raw data.
